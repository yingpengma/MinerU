import streamlit as st
import json
import time
import os
import base64
from datetime import datetime
from rag_core import initialize_rag_system, get_reference_map
from streamlit_pdf_viewer import pdf_viewer

# ä¸ºå®ç°å¯è¿½æº¯æ€§è€Œæ–°å¢çš„ import
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.core.callbacks.schema import CBEventType, EventPayload

# --- æ–°å¢: å¯å¤ç”¨çš„æº¯æºè¯¦æƒ…æ˜¾ç¤ºå‡½æ•° ---
def render_trace_expander(event_pairs, pdf_bytes, message_index):
    """æ ¹æ® event_pairs åœ¨ Streamlit ä¸­æ¸²æŸ“å¯å±•å¼€çš„æº¯æºè¯¦æƒ…ã€‚"""
    
    with st.expander("ğŸ” ç‚¹å‡»æŸ¥çœ‹æœ¬æ¬¡æŸ¥è¯¢çš„è¯¦ç»†æº¯æºæµç¨‹", expanded=False):
        if not event_pairs:
            st.warning("æœªèƒ½æ•è·åˆ°ä»»ä½•å¤„ç†äº‹ä»¶ã€‚")
            return

        # --- Helper to get duration and format it ---
        def get_duration(etype):
            pair = next((p for p in event_pairs if p[0].event_type == etype), None)
            if pair:
                time_format = "%m/%d/%Y, %H:%M:%S.%f"
                start_time = datetime.strptime(pair[0].time, time_format)
                end_time = datetime.strptime(pair[1].time, time_format)
                return (end_time - start_time).total_seconds()
            return None
        
        def get_duration_str(etype):
            duration = get_duration(etype)
            if duration is not None:
                return f" - è€—æ—¶: {duration:.4f} ç§’"
            return ""

        query_duration = get_duration(CBEventType.QUERY)
        if query_duration is not None:
            st.info(f"**æ€»æŸ¥è¯¢è€—æ—¶: {query_duration:.4f} ç§’**")

        # æ­¥éª¤ 1: é—®é¢˜å‘é‡åŒ–
        embedding_events = [p for p in event_pairs if p[0].event_type == CBEventType.EMBEDDING]
        if embedding_events:
            st.markdown("---")
            st.subheader(f"ç¬¬ä¸€æ­¥: é—®é¢˜å‘é‡åŒ– (Embedding){get_duration_str(CBEventType.EMBEDDING)}")
            st.markdown("ç³»ç»Ÿè°ƒç”¨ Embedding æ¨¡å‹ï¼Œå°†æ‚¨çš„æ–‡æœ¬é—®é¢˜è½¬æ¢æˆæœºå™¨å¯ä»¥ç†è§£çš„æ•°å­—åˆ—è¡¨ï¼ˆå‘é‡ï¼‰ã€‚")
            query_text = embedding_events[0][1].payload.get(EventPayload.CHUNKS, ["æœªçŸ¥è¾“å…¥"])[0]
            embedding_vector = embedding_events[0][1].payload.get(EventPayload.EMBEDDINGS, [[]])[0]
            st.text_area("è¾“å…¥æ–‡æœ¬", value=query_text, height=100, disabled=True, key=f"embed_in_{message_index}")
            st.text_area("è¾“å‡ºå‘é‡ (ä»…æ˜¾ç¤ºå‰5ä¸ªç»´åº¦)", value=str(embedding_vector[:5])+"...", height=100, disabled=True, key=f"embed_out_{message_index}")

        # æ­¥éª¤ 2: å‘é‡æ£€ç´¢
        retrieval_events = [p for p in event_pairs if p[0].event_type == CBEventType.RETRIEVE]
        if retrieval_events:
            st.markdown("---")
            st.subheader(f"ç¬¬äºŒæ­¥: å‘é‡æ£€ç´¢ (Retrieval){get_duration_str(CBEventType.RETRIEVE)}")
            st.markdown('ç³»ç»Ÿä½¿ç”¨"é—®é¢˜å‘é‡"ï¼Œåœ¨ ChromaDB ä¸­æœç´¢è¯­ä¹‰æœ€ç›¸å…³çš„æ–‡æœ¬å—ã€‚')
            retrieved_nodes = retrieval_events[0][1].payload.get(EventPayload.NODES, [])
            if retrieved_nodes:
                st.success(f"åœ¨æ•°æ®åº“ä¸­æˆåŠŸæ£€ç´¢åˆ° {len(retrieved_nodes)} ä¸ªç›¸å…³æ–‡æœ¬å—ã€‚")
                for node_idx, node in enumerate(retrieved_nodes):
                    with st.container(border=True):
                        page_num = node.metadata.get("page")
                        page_info = f" | åŸå§‹é¡µç : {page_num}" if page_num is not None else ""
                        st.markdown(f"**æ–‡æœ¬å— {node_idx+1} (ç›¸ä¼¼åº¦: {node.score:.4f}{page_info})**")
                        
                        if page_num is not None and pdf_bytes:
                            button_key = f"scroll_btn_{message_index}_{node.node_id}"
                            if st.button(f"ğŸ“œ æŸ¥çœ‹åŸæ–‡ç¬¬ {page_num} é¡µ", key=button_key):
                                st.session_state.scroll_to_page = page_num + 1
                                st.rerun()

                        st.text_area(f"å†…å®¹ (Chunk ID: {node.metadata.get('chunk_id')})", value=node.get_content(), height=120, disabled=True, key=f"retrieved_{message_index}_{node_idx}")

        # æ­¥éª¤ 3: ç­”æ¡ˆç”Ÿæˆ
        synthesize_events = [p for p in event_pairs if p[0].event_type == CBEventType.SYNTHESIZE]
        if synthesize_events:
            st.markdown("---")
            st.subheader(f"ç¬¬ä¸‰æ­¥: ç­”æ¡ˆç”Ÿæˆ (LLM Synthesize){get_duration_str(CBEventType.SYNTHESIZE)}")
            st.markdown('ç³»ç»Ÿå°†æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ä½œä¸º"ä¸Šä¸‹æ–‡"ï¼Œè¿åŒåŸå§‹é—®é¢˜ç»„åˆæˆæœ€ç»ˆæç¤ºè¯ (Prompt)ï¼Œå‘é€ç»™å¤§è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆç­”æ¡ˆã€‚')
            
            llm_duration = get_duration(CBEventType.LLM)
            if llm_duration is not None:
                st.caption(f"å…¶ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹è°ƒç”¨ (LLM) è€—æ—¶ {llm_duration:.4f} ç§’")

            llm_events = [p for p in event_pairs if p[0].event_type == CBEventType.LLM]
            if llm_events:
                prompt_payload = llm_events[0][0].payload.get(EventPayload.PROMPT) or llm_events[0][0].payload.get(EventPayload.MESSAGES)
                response_payload = llm_events[0][1].payload.get(EventPayload.RESPONSE) or llm_events[0][1].payload.get(EventPayload.LLM_RESPONSE)
                
                if prompt_payload:
                    full_prompt_str = "\n\n---\n\n".join([str(p) for p in prompt_payload]) if isinstance(prompt_payload, list) else str(prompt_payload)
                    st.text_area("å‘é€ç»™å¤§æ¨¡å‹çš„æœ€ç»ˆæç¤ºè¯ (Full Prompt)", value=full_prompt_str, height=400, disabled=True, key=f"prompt_{message_index}")
                
                if response_payload:
                    raw_response_str = str(getattr(response_payload, 'raw', response_payload))
                    st.text_area("ä»å¤§æ¨¡å‹æ”¶åˆ°çš„åŸå§‹å›å¤ (Raw Response)", value=raw_response_str, height=200, disabled=True, key=f"response_{message_index}")


# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Mineru RAG System",
    page_icon="â›ï¸",
    layout="wide"
)

st.title("â›ï¸ Mineru RAG ç³»ç»Ÿ")
st.write("ä¸€ä¸ªå®Œå…¨é€æ˜çš„äº¤äº’å¼é—®ç­”ç³»ç»Ÿã€‚æ‚¨å°†çœ‹åˆ°ä»æé—®åˆ°å›ç­”çš„æ¯ä¸€ä¸ªå†…éƒ¨æ­¥éª¤ã€‚")

# --- æ–°å¢: å®šä¹‰PDFæ–‡ä»¶è·¯å¾„å’ŒåŠ è½½å‡½æ•° ---
PDF_PATH = "output/demo2/auto/demo2_origin.pdf"

@st.cache_data
def get_pdf_bytes(path):
    if os.path.exists(path):
        with open(path, "rb") as f:
            return f.read()
    return None

pdf_bytes = get_pdf_bytes(PDF_PATH)

# --- æ–°å¢: åˆå§‹åŒ– session state ç”¨äºPDFæ»šåŠ¨ ---
if "scroll_to_page" not in st.session_state:
    st.session_state.scroll_to_page = None

# --- STEP 1: åŠ è½½å¹¶ç¼“å­˜åº•å±‚çš„ã€ç¨³å®šçš„ vector_store ---
with st.spinner("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿï¼ŒåŠ è½½æ¨¡å‹å’Œç´¢å¼•... è¯·ç¨å€™..."):
    vector_store = initialize_rag_system()
    reference_map = get_reference_map()

if vector_store is None:
    st.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")
    st.stop()

# --- æ–°å¢: åˆ›å»ºåŒæ å¸ƒå±€ ---
main_col, pdf_col = st.columns([6, 4])

# --- åœ¨å³ä¾§æ æ˜¾ç¤ºPDF ---
with pdf_col:
    if pdf_bytes:
        st.subheader("ğŸ“„ æ–‡æ¡£é¢„è§ˆ")
        pdf_viewer(
            input=pdf_bytes, 
            width=700, 
            height=800, 
            scroll_to_page=st.session_state.scroll_to_page
        )
        # æŸ¥çœ‹åé‡ç½®ï¼Œé˜²æ­¢åç»­è‡ªåŠ¨æ»šåŠ¨
        if st.session_state.scroll_to_page is not None:
            st.session_state.scroll_to_page = None
    else:
        st.warning("æœªèƒ½æ‰¾åˆ°ç”¨äºé¢„è§ˆçš„PDFæ–‡ä»¶ã€‚")


# --- å°†æ‰€æœ‰èŠå¤©ç›¸å…³UIæ”¾å…¥å·¦ä¾§æ  ---
with main_col:
    # --- èŠå¤©ç•Œé¢ ---
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘å·²ç»å­¦ä¹ äº†æ‚¨çš„æ–‡æ¡£ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"}]

    # --- ä¿®æ”¹: æ˜¾ç¤ºå®Œæ•´çš„èŠå¤©è®°å½•å’Œæº¯æºä¿¡æ¯ ---
    for idx, msg in enumerate(st.session_state.messages):
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æº¯æºæ•°æ®ï¼Œåˆ™æ¸²æŸ“å®ƒ
            if msg["role"] == "assistant" and "trace_events" in msg:
                render_trace_expander(msg["trace_events"], pdf_bytes, idx)

    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        
        # --- ä¿®æ”¹: æé—®åç«‹åˆ»åˆ·æ–°ï¼Œä»¥æ˜¾ç¤ºæ–°é—®é¢˜ ---
        st.rerun()

# --- ä¿®æ”¹: å°†æŸ¥è¯¢é€»è¾‘ç§»åˆ°ä¸»è„šæœ¬æµç¨‹ä¸­ ---
# æ£€æŸ¥æœ€åä¸€ä¸ªæ¶ˆæ¯æ˜¯å¦æ˜¯ç”¨æˆ·æ¶ˆæ¯ä¸”æ²¡æœ‰è¢«å¤„ç†è¿‡
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user" and "trace_events" not in st.session_state.messages[-1]:
    
    prompt = st.session_state.messages[-1]["content"]

    with main_col:
        with st.chat_message("assistant"):
            response_container = st.empty()
            response_container.write("æ€è€ƒä¸­...")

            # a. ä¸ºæœ¬æ¬¡æŸ¥è¯¢åˆ›å»ºå…¨æ–°çš„è¿½è¸ªå™¨
            llama_debug = LlamaDebugHandler()
            callback_manager = CallbackManager([llama_debug])
            
            # b. å°†è¿½è¸ªå™¨è®¾ç½®åœ¨å…¨å±€ Settings å¯¹è±¡ä¸Š
            Settings.callback_manager = callback_manager
            
            # c. ä»ç¼“å­˜çš„ vector_store åŠ¨æ€åˆ›å»º index
            index = VectorStoreIndex.from_vector_store(vector_store)
            
            # d. åˆ›å»ºæŸ¥è¯¢å¼•æ“
            query_engine = index.as_query_engine(similarity_top_k=3)
            
            with st.status("æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...", expanded=False) as status:
                status.write("1. åˆ†æé—®é¢˜å¹¶è°ƒç”¨ Embedding æ¨¡å‹...")
                response = query_engine.query(prompt)
                status.write("2. åœ¨ ChromaDB ä¸­æ‰§è¡Œå‘é‡æ£€ç´¢...")
                time.sleep(0.2)
                status.write("3. å°†æ£€ç´¢ç»“æœä¸é—®é¢˜æ•´åˆæˆæç¤ºè¯ (Prompt)...")
                time.sleep(0.2)
                status.write("4. è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆç­”æ¡ˆ...")
                time.sleep(0.2)
                status.update(label="ç­”æ¡ˆç”Ÿæˆå®Œæ¯•ï¼", state="complete")

            # --- ä¿®æ”¹: å°†æº¯æºæ•°æ®å­˜å…¥ session_state ---
            event_pairs = llama_debug.get_event_pairs()
            
            assistant_message = {
                "role": "assistant",
                "content": response.response,
                "trace_events": event_pairs
            }
            st.session_state.messages.append(assistant_message)
            
            # æœ€åï¼Œé‡ç½®å…¨å±€å›è°ƒç®¡ç†å™¨
            Settings.callback_manager = CallbackManager([])
            
            # --- ä¿®æ”¹: å¤„ç†å®Œåå†æ¬¡åˆ·æ–°ä»¥æ˜¾ç¤ºå®Œæ•´ç»“æœ ---
            st.rerun()
